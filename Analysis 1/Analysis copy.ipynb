{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9078ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('Final_Dataset.csv')\n",
    "\n",
    "# Exploratory Data Analysis\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nChurn Distribution:\")\n",
    "print(df['ChurnStatus'].value_counts())\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Prepare features and target\n",
    "X = df.drop('ChurnStatus', axis=1)\n",
    "y = df['ChurnStatus']\n",
    "\n",
    "# Handle class imbalance using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled\n",
    ")\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'XGBoost': XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "    'SVM': SVC(probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "# Model training and evaluation\n",
    "results = {}\n",
    "best_model = None\n",
    "best_score = 0\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = model.score(X_test, y_test)\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'auc': auc_score,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"{name} - Accuracy: {accuracy:.4f}, AUC: {auc_score:.4f}\")\n",
    "    \n",
    "    if auc_score > best_score:\n",
    "        best_score = auc_score\n",
    "        best_model = name\n",
    "\n",
    "print(f\"\\n*** Best Model: {best_model} with AUC: {best_score:.4f} ***\")\n",
    "\n",
    "# Hyperparameter tuning for the best model\n",
    "if best_model == 'XGBoost':\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.8, 0.9, 1.0]\n",
    "    }\n",
    "elif best_model == 'Random Forest':\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "else:  # Gradient Boosting as default\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 4, 5]\n",
    "    }\n",
    "\n",
    "print(f\"\\nPerforming hyperparameter tuning for {best_model}...\")\n",
    "grid_search = GridSearchCV(\n",
    "    models[best_model], param_grid, cv=5, scoring='roc_auc', n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best tuned model\n",
    "best_tuned_model = grid_search.best_estimator_\n",
    "y_pred_tuned = best_tuned_model.predict(X_test)\n",
    "y_pred_proba_tuned = best_tuned_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Tuned {best_model} AUC: {roc_auc_score(y_test, y_pred_proba_tuned):.4f}\")\n",
    "\n",
    "# Feature Importance Analysis\n",
    "if hasattr(best_tuned_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': best_tuned_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=feature_importance.head(10), x='importance', y='feature')\n",
    "    plt.title('Top 10 Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Detailed evaluation\n",
    "print(\"\\n=== Detailed Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred_tuned))\n",
    "\n",
    "print(\"\\n=== Confusion Matrix ===\")\n",
    "cm = confusion_matrix(y_test, y_pred_tuned)\n",
    "print(cm)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "for name, result in results.items():\n",
    "    fpr, tpr, _ = precision_recall_curve(y_test, result['probabilities'])\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC = {result[\"auc\"]:.3f})')\n",
    "\n",
    "fpr_tuned, tpr_tuned, _ = precision_recall_curve(y_test, y_pred_proba_tuned)\n",
    "plt.plot(fpr_tuned, tpr_tuned, label=f'Tuned {best_model} (AUC = {roc_auc_score(y_test, y_pred_proba_tuned):.3f})', \n",
    "         linewidth=2, linestyle='--')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves Comparison')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Business metrics\n",
    "def calculate_business_metrics(y_true, y_pred, y_pred_proba):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall (Sensitivity)': recall,\n",
    "        'Specificity': specificity,\n",
    "        'F1-Score': f1,\n",
    "        'AUC': roc_auc_score(y_true, y_pred_proba)\n",
    "    }\n",
    "\n",
    "business_metrics = calculate_business_metrics(y_test, y_pred_tuned, y_pred_proba_tuned)\n",
    "print(\"\\n=== Business Metrics ===\")\n",
    "for metric, value in business_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Save the best model\n",
    "import joblib\n",
    "joblib.dump(best_tuned_model, 'best_churn_model.pkl')\n",
    "print(\"\\nBest model saved as 'best_churn_model.pkl'\")\n",
    "\n",
    "# Prediction function for new data\n",
    "def predict_churn_probability(new_customer_data, model=best_tuned_model):\n",
    "    \"\"\"\n",
    "    Predict churn probability for new customer data\n",
    "    \"\"\"\n",
    "    if isinstance(new_customer_data, pd.DataFrame):\n",
    "        probabilities = model.predict_proba(new_customer_data)\n",
    "        predictions = model.predict(new_customer_data)\n",
    "        \n",
    "        results_df = new_customer_data.copy()\n",
    "        results_df['Churn_Probability'] = probabilities[:, 1]\n",
    "        results_df['Churn_Prediction'] = predictions\n",
    "        results_df['Risk_Level'] = pd.cut(probabilities[:, 1], \n",
    "                                        bins=[0, 0.3, 0.7, 1], \n",
    "                                        labels=['Low', 'Medium', 'High'])\n",
    "        \n",
    "        return results_df\n",
    "    else:\n",
    "        raise ValueError(\"Input should be a pandas DataFrame\")\n",
    "\n",
    "# Example usage (commented out as we don't have new data)\n",
    "# new_customers = pd.DataFrame({...})  # Your new data with same features\n",
    "# churn_predictions = predict_churn_probability(new_customers)\n",
    "# print(churn_predictions[['Churn_Probability', 'Churn_Prediction', 'Risk_Level']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
